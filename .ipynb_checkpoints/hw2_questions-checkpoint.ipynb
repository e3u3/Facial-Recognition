{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE152B: Homework 2\n",
    "## Computing Resources\n",
    "Please read the README file of this repository for the instructions\n",
    "## Instructions\n",
    "1. Attempt all questions.\n",
    "2. Please comment all your code adequately.\n",
    "3. Include all relevant information such as text answers, output images in notebook.\n",
    "4. **Academic integrity:** The homework must be completed individually.\n",
    "\n",
    "5. **Submission instructions:**  \n",
    " (a) Submit the notebook and its PDF version on Gradescope.  \n",
    " (b) Rename your submission files as Lastname_Firstname.ipynb and Lastname_Firstname.pdf.  \n",
    " (c) Correctly select pages for each answer on Gradescope to allow proper grading.\n",
    "\n",
    "6. **Due date:** Assignments are due Thu, May 21, at 4pm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Git clone the repo of HW2:\n",
    "```\n",
    "git clone https://github.com/Jerrypiglet/cse152b_hw2-release.git\n",
    "cd cse152b_hw2-release\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Using SphereFace [3] for Face Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the first section, we will test a pretrained model of SphereFace on LFW [4] dataset. The LFW dataset is on `/datasets/cse152-252-sp20-public/hw2_data/lfw`. The dataset contains 6000 pairs of human face images with ground truth labels for whether they are from the same identity.\n",
    "2. The PyTorch code of SphereFace is located in `./sphereFace`,  which is modified based on the open source code from `https://github.com/clcarwin/sphereface_pytorch`. \n",
    "3. Run the following commands and report the accuracy of SphereFace on LFW verification. **(5 points)**\n",
    "```\n",
    "cd sphereFace\n",
    "tar -zxf model.tar.gz\n",
    "python lfw_eval.py --model ./model/sphere20a_20171020.pth --net faceNet --lfw /datasets/cse152-252-sp20-public/hw2_data/lfw/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``  \n",
    "LFWACC=0.9918 std=0.0051 thd=0.3095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain briefly how the following steps are performed when evaluating on LFW dataset: \n",
    "    1. Given the features extracted from the network, what is the metric used to measure the distance between two faces? (`lfw_eval.py`: Line 135) **(5 points)**  \n",
    "    Here we are using the Angular Softmax that generates a softmax loss function that is normalized. Therefore the usual exponent $W^T x$ (assuming the bias is 0), becomes $||x||cos(m \\theta)$. In other words it is using an angular margin between classes.     \n",
    "    2. How is the threshold set to determine whether two faces are from the same identity? How is the accuracy computed? (`lfw_eval.py`: Line 141 to 148) **(10 points)**.  \n",
    "    First both images are concatenated together via np.vstack and given to the network together. The output is a cosine distance between the two faces as explained in part 1 above.\n",
    "    From the output predictions from the network, we find the best threshold from the list of (-1, 1, 0.005) by running all the predictions distances against these, and finding which threshold yields the highest accuracy rates. After finding this nice threshold, we run all the predictions for this kfold iteration on that chosen threshold and get an accuracy for this kfold iteration. \n",
    "    Accuracy is computed how it normally is : The number of true positives over the number of total predictions.   \n",
    "    We use the K-fold evaluation of validation in order to get the accuracy after training. We divide the training data into K folds, and alternate where each fold in the K folds act as the validation set each iteration. After repeating the above to all the kfolds, we get the average accuracy overall the kfold iterations.  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. An important step before face recognition is face alignment, in which we warp and crop the image based on the location of facial landmarks.\n",
    "    1. Briefly describe how we warp and crop the image. (`lfw_eval.py`: Line 11-26) **(5 points)**   \n",
    "    We have a preset array of 2d reference points of a face that we want the facial landmark features to be in at the end called ref_pts and a designated crop size that we want the image to be in. The given parameter src_pts (all pre processed in data/lfw_landmark.txt) tells us where these \"facial landmarks\" for each face are. We then calculate the affine transformation for the given src_points to line up with the ref_pts as well as possible (also while cropping the face into the specified size) using the function get_similarity_transform_for_PIL. We then return this new augmented face.  \n",
    "    \n",
    "    2. Instead of doing face alignment, crop an image patch of height 112 pixels and width 96 pixels at the center of the image. Report the accuracy.  **(10 points)**  \n",
    "    LFWACC=0.8748 std=0.0221 thd=0.2300\n",
    "    An observation that I would like to point out is that making this a simpler pre processing step, has decreased the training time by many times, but has lead to horrible accuracies due to the inefficient pre procressing augmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Using MTCNN [5] for Detecting Face Landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instead of using provided facial landmarks, we will now use MTCNN [5] for detecting them. The code is located at `./mtcnn`. Run the following commands to generate the facial landmarks. Include two example outputs in your report. **(5 points)**\n",
    "```\n",
    "cd MTCNN \n",
    "python lfw_landmark.py --lfw /datasets/cse152-252-sp20-public/hw2_data/lfw/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zurab_Tsereteli/Zurab_Tsereteli_0001.jpg        100.228 114.919 143.161 110.304 120.893 139.357 108.622      159.118 148.102 155.127\n",
    "Zydrunas_Ilgauskas/Zydrunas_Ilgauskas_0001.jpg  109.997 115.973 144.402 112.668 133.919 129.575 119.971      159.418 144.310 156.468"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Go to the `sphereFace` directory, run the following commands by setting flag `alignmentMode` to be 2. Report the error using the predicted facial landmarks. **(5 points)**\n",
    "```\n",
    "cd sphereFace\n",
    "python lfw_eval.py --model ./model/sphere20a_20171020.pth --net faceNet --lfw /datasets/cse152-252-sp20-public/hw2_data/lfw/ --alignmentMode 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LFWACC=0.9845 std=0.0057 thd=0.2995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next, answer the following questions:\n",
    "    1. Is the result better than using the landmarks provided in the previous question? If not, how can you improve performance? **(5 points)**  \n",
    "    \n",
    "    One of the problems is that if the image detects more than one face in the image, it just chooses the largest face in the array of landmarks. If we can hand crop the faces so that there are only one face per image, this can dramatically increase the 'clean-ness' of the dataset and yield higher accuracy rates. \n",
    "    2.  What are the steps adopted by the method to achieve real-time speed? **(5 points)**  \n",
    "    Uses Cuda network inferencing for one thing. In addition, they downscale the faces in lines \n",
    "    3. Briefly describe how non-maximal suppression (NMS) is implemented in this method. (`src/box_utils.py`: Line 5-68) **(5 points)**  \n",
    "    NMS attempts to limit the amount of redundant bounding boxes over the same object in the image.  \n",
    "    It will start from the largest box, and calculate the intersection between itself and the rest (smaller) boxes. It will union or take the minimum box (depending on the option) and delete boxes in which the overlap is so large with the current box that they are not taken into consideration (This threshold starts at 0.5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Training CosFace [7] on CASIA Dataset [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are required to implement CosFace based on the code you use in the previous section, train it on CASIA dataset and test on LFW dataset. \n",
    "1. You will now train CosFace [7] on the CASIA dataset [6] and test on the LFW dataset [4]. In this section, the skeleton code for training is given. CASIA-Webface dataset can be found at `/datasets/cse152-252-sp20-public/hw2_data/CASIA-WebFace`.  \n",
    "\n",
    "Go to directory `./cosFace` and open `faceNet.py`. Under class `CustomLienar` implement function $\\psi(\\theta_{y_i}, i)  = \\cos(\\theta_{y_i}, i) - m$ which is the cosine distance with margin $m$.\n",
    "Under class `CustomLoss`, implement the loss function with the returned $\\psi(\\theta_{y_i}, i)$ and $\\cos(\\theta_{j}, i)$. You may check (and duly cite) any open source implementation for hints on improving the performance. **(15 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Copy your implementation of CustomLinear and CustomLoss here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The original architecture of FaceNet in ``cosFace/FaceNet.py`` is a 20-layer residual network as described in Table 2 of [3], but without batch normalization. Now add batch normalization after every convolutional and fully connected layer. Train the new network on CASIA dataset and test on LFW dataset. You are free to change any hyper parameters but report the hyper parameters that you think might influence the performance. Following is a demonstration of a residual block with 128 filters and kernel size $3\\times3$:\n",
    "\\begin{align}\n",
    "y &=& \\mathtt{CONV}_{3\\times3, 128}(x) \\\\\n",
    "y &=& \\mathtt{BatchNorm}(y) \\\\\n",
    "y &=& \\mathtt{PReLU}(y) \\\\\n",
    "y &=& \\mathtt{CONV}_{3\\times3, 128}(y) \\\\\n",
    "y &=& \\mathtt{BatchNorm}(y) \\\\\n",
    "y &=& \\mathtt{PReLU}(y) \\\\\n",
    "\\mathtt{OUT} &=& x + y\n",
    "\\end{align}\n",
    "    1. Draw the training curves for accuracy and loss on CASIA and compare to the curve without batch normalization. **(10 points)**\n",
    "    2. Report accuracy on the LFW dataset, evaluated using `lfw_eval.py`. **(10 points)**\n",
    "    3. Do you achieve better performance on LFW? If yes, explain how batch normalization helps. If not, try to explain why the results are worse.  **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If you achieve better performance compared to SphereFace in Q1, well done! Can you provide a reason? If you do not outperform SphereFace, can you provide a cause? **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the tSNE visualization of the CosFace embedding for the same identities from the CASIA dataset used to visualize the SphereFace embedding in the previous question. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Human Pose Estimation with Convolutional Pose Machines (CPM) [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will be given code adapted from [PyTorch implementation of one of the CPM model [1]](https://github.com/Hzzone/pytorch-openpose), which is an follow-up work of the original Convolution Pose Machine [2], and shares the Part Confidence Maps estimation module with [2]. In this question you will be given a trained CPM model and gain insights about the model design and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ``cd pytorch-openpose``\n",
    "- Set up the environment following the instructions at ``pytorch-openpose/README.md`` (Getting Started - Install Requriements)\n",
    "- Download the ``body_pose_model.pth`` from [Dropbox](https://www.dropbox.com/sh/7xbup2qsn7vvjxo/AABWFksdlgOMXR_r5v3RwKRYa?dl=0) and place under ``pytorch-openpose/model``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the inference code on the given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from src import model\n",
    "from src import util\n",
    "from src.body import Body\n",
    "from src.hand import Hand\n",
    "\n",
    "body_estimation = Body('model/body_pose_model.pth')\n",
    "\n",
    "test_image = 'images/demo.jpg'\n",
    "oriImg = cv2.imread(test_image)  # B,G,R order\n",
    "candidate, subset, heatmap_list, heatmap_list_converted_list = body_estimation(oriImg)\n",
    "heatmap_0 = heatmap_list[0] \n",
    "canvas = copy.deepcopy(oriImg)\n",
    "canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(canvas[:, :, \n",
    "                  [2, 1, 0]])\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Visualize the output keypoint detection result. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste the output figure here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What is the ratio of size $\\lambda=H/H'$ between the input image **im** (Line 57 of ``body.py``) \n",
    "of shape [1, 3, H, W] and the output heatmap **heatmap_0** of shape [1, D, H', W']? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer the question here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) What module in the model is reponsible for this scaling? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer the question here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ``heatmap_list_converted_list[0]`` is a list of heatmaps from all 6 layers: [out1_2, out2_2, out3_2, out4_2, out5_2, out6_2], where the output of each layer is of shape [H, W, D]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet gives the function to visualize the $d_{th}$ feature map from the layer **layer_idx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer_idx = -1\n",
    "\n",
    "for d in range(heatmap_list_converted_list[0][layer_idx].shape[2]):\n",
    "    layer_idx = 0\n",
    "    heatmap = heatmap_list_converted_list[0][layer_idx][:, :, d]\n",
    "    util.overlay_heatmap(canvas, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What is D? And given D can you tell what is the number of keypoints that the model is trying to estimate? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Visualize and compare the heatmap from **layer_idx=0** and **layer_idx=5** for the keypoint **d=2**. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste and compare the visualizations here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Visualize and compare the heatmap from **layer_idx=0** and **layer_idx=5** for the keypoint **d=3**. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste and compare the visualizations here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) State and justify the shared difference in these comparisons? You may get some hint from the model design. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``State and justify the difference here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set **multi_scale=False** in Line 38 of ``body.py`` and re-run the code from Q4(1). \n",
    "\n",
    "    (a) Visualize the output below. **(5 points)**\n",
    "    \n",
    "    **NOTE: you might need to restart the kernel each time you change the ``multi_scale`` flag. This may be required to re-initialize the model. [Kernel]-->[Restart]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste figure by re-running (a) with multi_scale=False here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Do the same comparison with additional images of ``demo_coco.jpg``, ``demo_coco2.jpg``, ``demo_beach.jpg``, ``demo_ucsd.jpg``, ``demo_ucsd2.jpg``.  **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``For each of the additional images, paste two figures with multi_scale=True and multi_scale=False here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) What shared difference can you spot in most of the comparisons? Justify the difference. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Report the difference and jsutify here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "[1] Cao, Zhe. et al. \"Realtime multi-person 2d pose estimation using part affinity fields.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n",
    "\n",
    "[2] Wei, Shih-En. et al. \"Convolutional pose machines.\" Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2016.\n",
    "\n",
    "[3] Liu, Weiyang. et al. \"SphereFace: Deep Hypersphere Embedding for Face Recognition.\" arXiv:1704.08063.\n",
    "\n",
    "[4] Huang, Gary. et al. \"Labeled faces in the wild:  Adatabase for studying face recognition in unconstrained environments.\" Technical Report 07-49, Universityof Massachusetts, Amherst, October 2007.\n",
    "\n",
    "[5] Zhang, Kaipeng. et al. \"Joint face detection and alignment usingmultitask cascaded convolutional networks.\" IEEE Signal Processing Letters, 23(10):1499–1503, 2016.\n",
    "\n",
    "[6] Yi, Dong. et al.  Learning face representation from scratch. arXiv:1411.7923.\n",
    "\n",
    "[7] Wang, Hao. et al.Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages 5265–5274, 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
